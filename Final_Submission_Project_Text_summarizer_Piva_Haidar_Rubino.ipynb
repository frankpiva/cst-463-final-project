{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Submission_Project_Text_summarizer_Piva_Haidar_Rubino.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zslp1me6euiS",
        "VgwnXmMgnWZL",
        "xSnrxQiWnEdN",
        "hoEgK3IF4DeG",
        "FaKec2mK5Wwg",
        "eR2h07fc5g8p",
        "3Q508-rzs1Hz",
        "rmgj17pMtMyi"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC5H_j_dVDUP"
      },
      "source": [
        "# Title: Not so Trashy Text Summarizer\r\n",
        "by Federico Rubino, Michael Haidar and Frank Piva\r\n",
        "\r\n",
        "The goal of our project is to create a Natural Language Processing (NLP) model for abstractive document summarization. \r\n",
        "Text summarization, creating concise and understandable summaries containing key information and general meaning, is broadly done in one of two approaches: extractive and abstractive summarization. Extractive summarization produces the most important text, verbatim, from the original document. Abstractive summarization generates new text from the original document (Allahyari, 2017). To produce such a system we will use a variant of Recurrent Neural Networks(RNN). The stretch goal is to generalize our system to summarize legal or medical documents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aAwZL5idZ6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872d53f3-cfaf-4996-f151-c5f5c3241952"
      },
      "source": [
        "# environment intialization\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import display, Markdown, Latex\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Layer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import re\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "\n",
        "nltk.download('punkt') # one time execution\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD7KXBmbVdqD"
      },
      "source": [
        "# Problem Statement\r\n",
        "In this notebook, we will build an abstractive based text summarizer using deep learning using keras\r\n",
        "\r\n",
        "We had a lot of influence from the following [article.](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/)\r\n",
        "\r\n",
        "We set up 6 inintial objectives to solve the issue of creating a neural net that could analyse a text and create a summary from it. The goal was to be able to feed it any text no matter the context and be able to create a summary or an abstract out of it. Overall this was intended to make us a lot more familiar with how abstract layers work, how to properly use tokenizers, and develop an intuition for RNNs.\r\n",
        "\r\n",
        "## Baseline Objectives/Phases:  \r\n",
        "- Phase 1:\r\n",
        "Understand the problem space\r\n",
        "Summarize possible RNNs to use\r\n",
        "Explore data ([trashy] Daily Mail)\r\n",
        "- Phase 2:\r\n",
        "Using the Amazon reviews to create a working model\r\n",
        "Code working RNN model\r\n",
        "Code text generator\r\n",
        "Generate some text for one article \r\n",
        "- Phase 3:\r\n",
        "Develop better understanding of our problem space\r\n",
        "Produce abstractive summaries\r\n",
        "- Phase 4:\r\n",
        "Test model with covid19sum dataset\r\n",
        "Fine Tuning\r\n",
        "- Phase 5:\r\n",
        "Find legal document dataset\r\n",
        "preprocess documents\r\n",
        "- Phase 6: \r\n",
        "Legal document summaries.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3OEgWsXXkBI"
      },
      "source": [
        "# Custom Attention Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTUS4-2IX6Zk"
      },
      "source": [
        "We had a lot of influence from the following [article](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) for creating this Attention layer.\r\n",
        "\r\n",
        "Introduced by Bahdanau et al., attention is a popular approach to focus resources on a specific set of most relevant elements. For instance, the background of an image may be of little importance when asking about the subject, but more important if asking about the setting. Rather than tailoring preprocessing or feature engeneering, computing the relevance of elements is weighed by the neural architecture (Galassi, et al. 2019).\r\n",
        "\r\n",
        "For NLP, attention is applied to either global or local source positions(ie: to a few paragraphs , sentence by sentence, or word by word) For abstractive text summarization of scientific articles global attention may be best applied to the entirety of the paper. \r\n",
        "\r\n",
        "How the attention layer works with the encoder and decoder:\r\n",
        "  - every time step the encoder ouputs the hidden state\r\n",
        "  - every time step the decoder outputs the hidden state\r\n",
        "  - an alignment score is calculated when comparing target and source word. pass the alignment scores through a softmax to get the attention weights\r\n",
        "      - softmax makes big numbers very big and negative numbers very small positive numbers. (e^x) the higher it predicts one value the lower it predicts others, this is sometimes refered to as attentuation, hence the name.\r\n",
        "  - We can use these weights to calculate \"context\" by doing a linear sum of products with the hidden states from the encoder\r\n",
        "  - concatenate the hidden states from the decoder with the context\r\n",
        "  - pass these through a dense layer to produce the output of the decoder which is then used for the proability of the next word\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrIFLR0A5QH5"
      },
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "    There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\" Step function for computing energy for a single decoder state\n",
        "            inputs: (batchsize * 1 * de_in_dim)\n",
        "            states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aoFU6H8bn1y"
      },
      "source": [
        "# Reading the Data set\r\n",
        "The COVID19 data comes with over 170,000 articles. Where some have an abstract or summary already included and some only have the article itself. The goal is then to train on the articles that come with a summary and then test on those that don't. This would be ideal for two reasons, one is that we could actually create a model that is trained on a sientific article and then if we create a model that is actually good at creating summaries we could populate those without a summary with one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-h3j5-cYMV"
      },
      "source": [
        "data0 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b1.csv')\r\n",
        "data1 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b2.csv')\r\n",
        "data2 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b3.csv')\r\n",
        "data3 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b4.csv')\r\n",
        "data4 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b5.csv')\r\n",
        "data5 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b6.csv')\r\n",
        "data6 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b7.csv')\r\n",
        "data7 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_b8.csv')\r\n",
        "data8 = pd.read_csv('https://raw.githubusercontent.com/frankpiva/cst-463-final-project/master/articles/data_bL.csv')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dahB1XROcZmr"
      },
      "source": [
        "frames = [data0,data1,data2,data3,data4,data5,data6,data7,data8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_IA82RVmfaw"
      },
      "source": [
        "data = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSz0USUvmhNe"
      },
      "source": [
        "del data0, data1, data2, data3, data4, data5, data6, data7, data8, frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXzQh1OBmi4o",
        "outputId": "20334c3a-db47-40de-9120-f2fd8c65eda7"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 12887 entries, 0 to 2045\n",
            "Data columns (total 5 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   Unnamed: 0       12887 non-null  int64 \n",
            " 1   Text             12887 non-null  object\n",
            " 2   Summary          12887 non-null  object\n",
            " 3   cleaned_text     12887 non-null  object\n",
            " 4   cleaned_summary  12887 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 604.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fwu18c_AcZ3W"
      },
      "source": [
        "## Data cleaning and formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT3WucJLcgmA"
      },
      "source": [
        "## Preprocessing\r\n",
        "Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\r\n",
        "\r\n",
        "Here is the dictionary that we will use for expanding the contractions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0hQr3kdckRn"
      },
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\r\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\r\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\r\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\r\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\r\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\r\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\r\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\r\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\r\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\r\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\r\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\r\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\r\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\r\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\r\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\r\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\r\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\r\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\r\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\r\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\r\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\r\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bpuA8G1csOK"
      },
      "source": [
        "We will perform the below preprocessing tasks for our data:\r\n",
        "\r\n",
        "1.Convert everything to lowercase\r\n",
        "\r\n",
        "2.Remove HTML tags\r\n",
        "\r\n",
        "3.Contraction mapping\r\n",
        "\r\n",
        "4.Remove (‘s)\r\n",
        "\r\n",
        "5.Remove any text inside the parenthesis ( )\r\n",
        "\r\n",
        "6.Eliminate punctuations and special characters\r\n",
        "\r\n",
        "7.Remove stopwords\r\n",
        "\r\n",
        "8.Remove short words\r\n",
        "\r\n",
        "Let’s define the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Cuu8PleriF"
      },
      "source": [
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english')) \r\n",
        "\r\n",
        "def text_cleaner(text,num):\r\n",
        "    newString = text.lower()\r\n",
        "    newString = BeautifulSoup(newString, \"lxml\").text\r\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\r\n",
        "    newString = re.sub('\"','', newString)\r\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \r\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\r\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \r\n",
        "    newString = re.sub('[m]{2,}', 'mm', newString)\r\n",
        "    if(num==0):\r\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\r\n",
        "    else:\r\n",
        "        tokens=newString.split()\r\n",
        "    long_words=[]\r\n",
        "    for i in tokens:\r\n",
        "        if len(i)>1:                                                 #removing short word\r\n",
        "            long_words.append(i)   \r\n",
        "    return (\" \".join(long_words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zslp1me6euiS"
      },
      "source": [
        "### Call the cleaning function on the dataset for both article and summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgwnXmMgnWZL"
      },
      "source": [
        "## Drop empty rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgwsiUAynaWI"
      },
      "source": [
        "data.replace('', np.nan, inplace=True)\r\n",
        "data.dropna(axis=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSnrxQiWnEdN"
      },
      "source": [
        "## Final Product:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoS2YiiLnCrd",
        "outputId": "98b29ca7-8d46-4c55-9816-8f1ed4413d1d"
      },
      "source": [
        "for i in range(2):\r\n",
        "  print(\"Article\", i+1)\r\n",
        "  print(data['cleaned_text'][i])\r\n",
        "  print('-'*100)\r\n",
        "  print(data['cleaned_summary'][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article 1\n",
            "0    use personal protective equipment central behavioural policy response control spread sars cov virus global covid pandemic particular masks sometimes gloves aprons gowns face eye protection recomme...\n",
            "0    document written request parisarea healthcare authorities france guidance intended help professionals coordinate patients pathways standardize practices among centers avoid acting first come first...\n",
            "0    historically pacific island countries territories severely affected influenza pandemics part world extreme example influenza pandemic western samoa experienced loss population hand also pandemic f...\n",
            "0    drug metabolism experiment played important role drug discovery drug design drug clinical application therefore fast efficient ways provide accurate information drug metabolism target compounds ma...\n",
            "0    date italy experiencing severe outbreak covid infection largely unexpected death rate consequence entire nation global lockdown contain virus spread since february except health services food supp...\n",
            "0    may researchers craig venter institute published science latest achievement creation first synthetic cell labeled jcvi syn controlled completely chemically synthesized genome achievement upon publ...\n",
            "0    recent research shown psychological stress influences susceptibility experimentally induced upper respiratory tract illnesses also shown respiratory virus infections commonly cause associated exac...\n",
            "0    insufficient properly resolve small scales mixing process crucial droplet evaporation also temperature humidity fields fully coupled however determine evaporation rate thus lifetime droplets essen...\n",
            "0    insufficient properly resolve small scales mixing process crucial droplet evaporation also temperature humidity fields fully coupled however determine evaporation rate thus lifetime droplets essen...\n",
            "Name: cleaned_text, dtype: object\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0    use of personal protective equipment has been central to controlling spread of sars cov this study aims to quantify the environmental impact of this and to model strategies for its reduction life ...\n",
            "0    sars cov has caused global pandemic unprecedented in size spread severity and mortality the influx of patients with severe or life threatening disease means that in some cases the available medica...\n",
            "0    background historically pacific island countries and territories have been more severely affected by influenza pandemics than any other part of the world we herein describe the emergence and epide...\n",
            "0    sensitive and specific method for the analysis of anisodamine and its metabolites in rat urine by liquid chromatography electrospray ionization tandem mass spectrometry was developed various extra...\n",
            "0    to contain covid spread italy is under global lockdown since february except for health services and food supply in this scenario growing apprehension concerning legal consequences is rising among...\n",
            "0    synthetic biology is an emerging field which since its birth has shown great value and potential in many fields including medicine energy environment and agriculture it is also important for the s...\n",
            "0    the aim of this research was study the role of psychosocial factors in exacerbations of asthma in adults induced by upper respiratory tract infections it involved longitudinal study of adults with...\n",
            "0    to mitigate the covid pandemic it is key to slow down the spreading of the life threatening coronavirus this spreading mainly occurs through virus laden droplets expelled at speaking screaming sho...\n",
            "0    to mitigate the covid pandemic it is key to slow down the spreading of the life threatening coronavirus this spreading mainly occurs through virus laden droplets expelled at speaking screaming sho...\n",
            "Name: cleaned_summary, dtype: object\n",
            "Article 2\n",
            "1    developed deterministic compartmental transmission model sars cov population stratified disease status disease awareness status due spread covid self imposed measures assumed taken diseaseaware in...\n",
            "1    coronavirus acute disease primarily targets respiratory system however mounting evidence suggests cardiac involvement common particularly hospitalized patients substantial increase morbidity morta...\n",
            "1    new coronavirus sars cov found cause pulmonary disease city wuhan china december name sars cov due genomic rna similar severe acute respiratory syndrome coronavirus viruses found link clade betaco...\n",
            "1    le virus influenza origine de la premi pand mie du xxi si cle les premiers cas sont apparus en france au du mois de mai pendant deux mois tous les cas ont hospitalis et isol la mise en place de la...\n",
            "1    outbreak middle east respiratory syndrome coronavirus posed serious public health threat republic korea generated confirmed cases including fatal cases drawn lot attention largest outbreak mers co...\n",
            "1    challenges combating novel coronavirus pandemic put halt many economic socio cultural activities many societies across globe hand triggered avalanche scientific research within outside medical dom...\n",
            "1    one approach estimate prevalence assay measurements begins setting cut value samples whose measurement exceeds value diagnosed infected uninfected rather setting cut model probability sample infec...\n",
            "1    measles virus member genus morbillivirus family paramyxoviridae order mononegavirales consists nucleotides encode six major structural proteins nucleoprotein phosphoprotein large protein genomic r...\n",
            "1    measles virus member genus morbillivirus family paramyxoviridae order mononegavirales consists nucleotides encode six major structural proteins nucleoprotein phosphoprotein large protein genomic r...\n",
            "Name: cleaned_text, dtype: object\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1    the coronavirus disease caused by the severe acute respiratory syndrome coronavirus has spread to nearly every country in the world since it first emerged in china in december many countries have ...\n",
            "1    tel word count conflict of interest none declared journal pre proof abstract coronavirus is an acute respiratory disease that has rapidly spread around the world and been declared global pandemic ...\n",
            "1    recently pathogen has been identified as novel coronavirus and found to trigger novel pneumonia in human beings and some other mammals the uncontrolled release of cytokines is seen from the primar...\n",
            "1    le registre reva grippe srlf permis de colliger les donn es de patients infect par le virus grippal et hospitalis en animation la mortalit globale de ces patients de le recours la ventilation inva...\n",
            "1    super spreading events have been observed in the transmission dynamics of many infectious diseases the mers cov outbreak in the republic of korea has also shown super spreading events with signifi...\n",
            "1    please cite this article as haghani bliemer goerlandt li the scientific literature on coronaviruses covid and its associated safety related research dimensions scientometric analysis and scoping r...\n",
            "1    during the emergence of pandemic we need to estimate the prevalence of disease using serological assays whose characterization is incomplete relying on limited validation data this introduces unce...\n",
            "1    further attenuated measles vaccines were developed more than years ago and have been used throughout the world recombinant measles vaccine candidates have been developed and express several hetero...\n",
            "1    further attenuated measles vaccines were developed more than years ago and have been used throughout the world recombinant measles vaccine candidates have been developed and express several hetero...\n",
            "Name: cleaned_summary, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfJIOcvDncQN"
      },
      "source": [
        "# Understanding the distribution of the sequences\r\n",
        "\r\n",
        "Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence. We can see that these articles can be very large. This proved to be one of the biggest challenges with the ADS. Colab and our personal computers were not strong enough to run the LSTM or a Bidirectional LSTM with the whole dataset. Infact, it was a struggle just to preprocess the corpa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "2C5hJ8xJrnqE",
        "outputId": "2f8d6723-dc92-45ff-fa95-5a361a31a1d6"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "text_word_count = []\r\n",
        "summary_word_count = []\r\n",
        "\r\n",
        "# populate the lists with sentence lengths\r\n",
        "for i in data['cleaned_text']:\r\n",
        "      text_word_count.append(len(i.split()))\r\n",
        "\r\n",
        "for i in data['cleaned_summary']:\r\n",
        "      summary_word_count.append(len(i.split()))\r\n",
        "\r\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\r\n",
        "\r\n",
        "length_df.hist(bins = 30)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvfoT14rqA2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoEgK3IF4DeG"
      },
      "source": [
        "### Figuring out the size of articles and summaries.\r\n",
        "This was used to test the limits of our model and computing power. We had to find the max we could train with, without crashing.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ADu1DR5SgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d036e687-95d1-4453-ddb8-463686760e34"
      },
      "source": [
        "#propotion of length of summaries\r\n",
        "cnt=0\r\n",
        "for i,j in zip(data['cleaned_summary'],data['cleaned_text']):\r\n",
        "    if(len(i.split())>=100 and len(i.split())<=300 \r\n",
        "      and len(j.split())>=200 and len(j.split())<=600):\r\n",
        "        cnt=cnt+1\r\n",
        "print(cnt/len(data['cleaned_summary']))\r\n",
        "\r\n",
        "#75% of the data has  100 < sum_lenth  and 800 < text_length\r\n",
        "#70% of the data has  100 < sum_length < 500 and 800 < text_length < 6000\r\n",
        "#65% of the data has  100 < sum_length < 500 and 800 < text_length < 4000\r\n",
        "#64% of the data has  100 < sum_length < 500 and 1000 < text_length < 5000\r\n",
        "#60% of the data has  100 < sum_length < 500 and 1000 < text_length < 4000\r\n",
        "#55% of the data has  100 < sum_length < 500 and 800 < text_length < 3000\r\n",
        "#51% of the data has  100 < sum_length < 500 and 1000 < text_length < 3000\r\n",
        "#47% of the data has  100 < sum_length < 500 and 800 < text_length < 2500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.03476371537208039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaKec2mK5Wwg"
      },
      "source": [
        "### Some more data prep \r\n",
        "Here we add End and Start of sentence tokens. This is used for the encoder and decoder to know when the begining and end of a sentence is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyZo4nyWriQA"
      },
      "source": [
        "max_text_len=600\r\n",
        "min_text_len=200\r\n",
        "\r\n",
        "max_summary_len=300\r\n",
        "min_summary_len=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47AIJtmrlM-"
      },
      "source": [
        "#selecting summaries of desired length\r\n",
        "cleaned_text =np.array(data['cleaned_text'])\r\n",
        "cleaned_summary=np.array(data['cleaned_summary'])\r\n",
        "\r\n",
        "short_text=[]\r\n",
        "short_summary=[]\r\n",
        "\r\n",
        "for i in range(len(cleaned_text)):\r\n",
        "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_summary[i].split())>=min_summary_len \r\n",
        "       and len(cleaned_text[i].split())<=max_text_len and len(cleaned_text[i].split())>=min_text_len):\r\n",
        "        short_text.append(cleaned_text[i])\r\n",
        "        short_summary.append(cleaned_summary[i])\r\n",
        "        \r\n",
        "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB_HOzbx5eds"
      },
      "source": [
        "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqQkUHaZp1X9",
        "outputId": "02d72b8b-1b4e-4bd1-a770-a23a870c3958"
      },
      "source": [
        "for i in range(1):\r\n",
        "  print(\"Article\", i+1)\r\n",
        "  print(df['text'][i])\r\n",
        "  print('-'*100)\r\n",
        "  print(df['summary'][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Article 1\n",
            "coronavirus disease pandemic outbreak started china patients pneumonia unknown etiology symptoms presented dry cough sore throat diarrhea etiologic agent identified rna virus belonging coronaviridae family named sars cov epidemic progressed manifestations observed recalcati et al studied patients admitted lecco hospital lombardy italy patients skin reactions eight lesions occurred hospitalization reactions divided authors three forms erythematous rash widespread urticaria chickenpox like vesicles addition study case reported joob et al thailand case patient presented rash low platelet count initially misdiagnosed dengue evolved respiratory symptoms referred tertiary hospital tested positive sars cov despite existence skin lesions descriptions little information evolutions pictures therefore report one case following since prodromal period resolution symptoms female patient years old intensive care physician resident city recife pe hypothyroidism overweight comorbidities patient contact intensive care unit patient previously tested positive sars cov five days later painful erythematous edematous plaques appeared flexor face forearms leg extensors lesions evolved bruises treated betamethasone cream day lesion resolution days patient second exposure another icu patient covid fever epistaxis headache myalgia vomiting diarrhea skin presented pruritic urticarial lesions shoulders inguinal region palms hands erythema intense itching medicated bilastine mg one tablet day days within wheals erythematous edematous plaques appeared without itching antecubital popliteal fossae lesions regressed use betamethasone ointment cream day days patient presented anosmia nasal obstruction asthenia mild dyspnea however return skin lesions laboratory showed positive polymerase chain reaction sars cov results within normality lactate dehydrogenase reactive protein mg fibrinogen mg dl international normalized ratio troponin ng ml dimer total leukocytes mm neutrophils eosinophils lymphocytes platelets mm alanine aminotransferase pg ml aspartate aminotransferase total bilirubin mg dl serum creatinine mg dl used oral zinc completely recovered days onset systemic symptoms pandemic situation caused covid still recent moment little information cutaneous conditions associated virus report joob et al serves warning health professionals possibility presence rash initial sign infection new coronavirus viral etiology often remembered investigation acute urticaria occurs rash however infections several virus families parvoviridae caliciviridae addition hepatitis viruses described causing acute chronic urticaria children adults currently using drug therapy acute urticaria increase il pcr dimer inflammatory factors greatly increased covid il possible immunological link reports onset urticaria second infection dengue epstein barr reactivation known new exposures coronavirus increased immune stimulus exacerbating lesions necessary use serological tests frequently assess viral etiology patients acute urticarial edematous lesions initial frustrating lesions excellent response antihistamine cortisone cream rapid change localized lesions practically asymptomatic would prevent patient tested covid systemic symptoms patients edematous skin lesions even classic ones suggestive viral etiology evaluated covid another virosis knowing cutaneous manifestations caused covid essential step early diagnosis disease helping decrease spread author contributions authors contributed study conception design material preparation data collection analysis performed vlsm first draft manuscript written lfts authors commented previous versions manuscript authors read approved final manuscript funding funding source\n",
            "----------------------------------------------------------------------------------------------------\n",
            "sostok the disease caused by the new coronavirus has many systemic manifestations affecting the upper airways lungs gastrointestinal tract and inducing hematological repercussions with the evolution of the pandemic skin lesions were observed however there is little information about the evolution of the lesions at this moment the authors report case of patient who had more than one exposure to the coronavirus during the evolution of the disease and manifested different types of edematous lesions the lesions started in the prodromal period and changed their presentation and localization during the evolution of covid the lesions regressed quickly with the use of corticoid cream and antihistamine viral skin lesions are frequent causes of exanthema however viral etiology is not always investigated in acute urticarial and atypical erythematous edematous conditions the immunological basis of acute urticaria has points in common with covid justifying the appearance of lesions investigation of viral etiology should always be remembered in acute urticarial and edematous condition eostok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eR2h07fc5g8p"
      },
      "source": [
        "### Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13SF2l6g5kMC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0D-u2dX5lR0"
      },
      "source": [
        "# Prep the Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA43LoKy5neh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer \r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "#prepare a tokenizer for text on training data\r\n",
        "x_tokenizer = Tokenizer() \r\n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEMs2X-95-PW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54909c5a-a8b3-408a-83db-a01f983a1e80"
      },
      "source": [
        "thresh=4\r\n",
        "\r\n",
        "cnt=0\r\n",
        "tot_cnt=0\r\n",
        "freq=0\r\n",
        "tot_freq=0\r\n",
        "\r\n",
        "for key,value in x_tokenizer.word_counts.items():\r\n",
        "    tot_cnt=tot_cnt+1\r\n",
        "    tot_freq=tot_freq+value\r\n",
        "    if(value<thresh):\r\n",
        "        cnt=cnt+1\r\n",
        "        freq=freq+value\r\n",
        "    \r\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\r\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 64.12434275148784\n",
            "Total Coverage of rare words: 9.223225144208778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ATOfizsF2a"
      },
      "source": [
        "#prepare a tokenizer for text on training data\r\n",
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \r\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\r\n",
        "\r\n",
        "#convert text sequences into integer sequences\r\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \r\n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\r\n",
        "\r\n",
        "#padding zero upto maximum length\r\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\r\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\r\n",
        "\r\n",
        "#size of vocabulary ( +1 for padding token)\r\n",
        "x_voc   =  x_tokenizer.num_words + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhpgXWzM5-W7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4bd0d8-3d27-48d7-97e7-1c8a4d18b039"
      },
      "source": [
        "print('number of words in body text learned by the tokenizer:', x_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words in body text learned by the tokenizer: 6210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4LQiQ4A5-ea"
      },
      "source": [
        "#prepare a tokenizer for summary on training data\r\n",
        "y_tokenizer = Tokenizer()   \r\n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQI85JgBsa08",
        "outputId": "0ab6601d-a3c8-423a-e64d-08d06fb23ca4"
      },
      "source": [
        "thresh=6\r\n",
        "\r\n",
        "cnt=0\r\n",
        "tot_cnt=0\r\n",
        "freq=0\r\n",
        "tot_freq=0\r\n",
        "\r\n",
        "for key,value in y_tokenizer.word_counts.items():\r\n",
        "    tot_cnt=tot_cnt+1\r\n",
        "    tot_freq=tot_freq+value\r\n",
        "    if(value<thresh):\r\n",
        "        cnt=cnt+1\r\n",
        "        freq=freq+value\r\n",
        "    \r\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\r\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 78.76085697741748\n",
            "Total Coverage of rare words: 17.527989502191694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPHeOv4ssbdJ"
      },
      "source": [
        "#prepare a tokenizer for summary on training data\r\n",
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \r\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\r\n",
        "\r\n",
        "#convert text sequences into integer sequences\r\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \r\n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \r\n",
        "\r\n",
        "#padding zero upto maximum length\r\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\r\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\r\n",
        "\r\n",
        "#size of vocabulary\r\n",
        "y_voc  =   y_tokenizer.num_words +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz1PDjAoshod",
        "outputId": "9eba5cff-1bba-4d23-e606-91c87c59130b"
      },
      "source": [
        "#y_voc size seems too small.\r\n",
        "#first try increasing number of articles\r\n",
        "print('number of words in summaries learned by the tokenizer:', y_voc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words in summaries learned by the tokenizer: 1835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9H7hJ-5-r8"
      },
      "source": [
        "# Build the Model\r\n",
        "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.\r\n",
        "\r\n",
        "Return Sequences = True: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\r\n",
        "\r\n",
        "Return State = True: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\r\n",
        "\r\n",
        "Initial State: This is used to initialize the internal states of the LSTM for the first timestep\r\n",
        "\r\n",
        "Stacked LSTM: Stacked LSTM has multiple layers of LSTM stacked on top of each other. This leads to a better representation of the sequence. I encourage you to experiment with the multiple layers of the LSTM stacked on top of each other (it’s a great way to learn this)\r\n",
        "\r\n",
        "Bidirectional layer: Rather than just have the model look at the present and past words, a bidirectional layer looks ahead before encoding. This is essentially learning future context. It acomplishes this by running the LSTM in both directions, from the start and the end of a phrase.\r\n",
        "\r\n",
        "The Latent Dimensions: (hidden)\r\n",
        "The latent space of a model can be understood as a compressed representation of the data. The latent space is similar to how convolutions recognize edges or angles, it compresses the input into its most important features (reducing demensionality). The rest of the information not held by the latent space is discarded and the decoder must then learn to recreate the input from this latent space. The latent \"space\" referes to the new, reduced, dimensions of the input, the \"space\" that it now populates if we were to graph it. The bigger the latent space the less compression is done and the more features of the input the model remembers. Our biggest issue in ADS is memory. We could not train with a large latenet space, which begs the question; how similar can scientific papers be to eachother?\r\n",
        "\r\n",
        "Here, we are building a 3 stacked LSTM for the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnsulg8Y6AvT",
        "outputId": "aa5cbec0-8426-4c3f-b9a2-1232ed9c2f76"
      },
      "source": [
        "from keras import backend as K \r\n",
        "K.clear_session()\r\n",
        "\r\n",
        "latent_dim = 200\r\n",
        "embedding_dim=100\r\n",
        "\r\n",
        "# Encoder\r\n",
        "encoder_inputs = Input(shape=(max_text_len,))\r\n",
        "\r\n",
        "#embedding layer\r\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\r\n",
        "\r\n",
        "#encoder lstm 1\r\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\r\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\r\n",
        "\r\n",
        "#encoder lstm 2\r\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\r\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\r\n",
        "\r\n",
        "#encoder lstm 3\r\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\r\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\r\n",
        "\r\n",
        "# Set up the decoder, using `encoder_states` as initial state.\r\n",
        "decoder_inputs = Input(shape=(None,))\r\n",
        "\r\n",
        "#embedding layer\r\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\r\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\r\n",
        "\r\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\r\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\r\n",
        "\r\n",
        "# Attention layer\r\n",
        "attn_layer = AttentionLayer(name='attention_layer')\r\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\r\n",
        "\r\n",
        "# Concat attention input and decoder LSTM output\r\n",
        "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\r\n",
        "\r\n",
        "#dense layer\r\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\r\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\r\n",
        "\r\n",
        "# Define the model \r\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\r\n",
        "\r\n",
        "model.summary() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 600)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 600, 100)     621000      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 600, 200), ( 240800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 600, 200), ( 320800      lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    183500      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 600, 200), ( 320800      lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 200),  240800      embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 200),  80200       lstm_2[0][0]                     \n",
            "                                                                 lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 400)    0           lstm_3[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 1835)   735835      concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 2,743,735\n",
            "Trainable params: 2,743,735\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvctdN2D6H-A"
      },
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11IKpVK06P8x"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3k2Vi1fH6Qkg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db821ea-3c3d-44cf-be8f-5a2f16106c2a"
      },
      "source": [
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=100,batch_size=16, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "26/26 [==============================] - 193s 7s/step - loss: 4.7524 - val_loss: 2.9166\n",
            "Epoch 2/100\n",
            "26/26 [==============================] - 169s 7s/step - loss: 3.1100 - val_loss: 2.8555\n",
            "Epoch 3/100\n",
            "26/26 [==============================] - 171s 7s/step - loss: 3.0428 - val_loss: 2.8515\n",
            "Epoch 4/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.9536 - val_loss: 2.8184\n",
            "Epoch 5/100\n",
            "26/26 [==============================] - 176s 7s/step - loss: 3.0719 - val_loss: 2.8148\n",
            "Epoch 6/100\n",
            "26/26 [==============================] - 173s 7s/step - loss: 2.9362 - val_loss: 2.8045\n",
            "Epoch 7/100\n",
            "26/26 [==============================] - 171s 7s/step - loss: 3.0130 - val_loss: 2.8053\n",
            "Epoch 8/100\n",
            "26/26 [==============================] - 180s 7s/step - loss: 2.9516 - val_loss: 2.7928\n",
            "Epoch 9/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.8559 - val_loss: 2.7729\n",
            "Epoch 10/100\n",
            "26/26 [==============================] - 173s 7s/step - loss: 2.9188 - val_loss: 2.7794\n",
            "Epoch 11/100\n",
            "26/26 [==============================] - 178s 7s/step - loss: 2.7733 - val_loss: 2.7640\n",
            "Epoch 12/100\n",
            "26/26 [==============================] - 174s 7s/step - loss: 2.7406 - val_loss: 2.7575\n",
            "Epoch 13/100\n",
            "26/26 [==============================] - 169s 6s/step - loss: 2.8085 - val_loss: 2.7514\n",
            "Epoch 14/100\n",
            "26/26 [==============================] - 175s 7s/step - loss: 2.7518 - val_loss: 2.7307\n",
            "Epoch 15/100\n",
            "26/26 [==============================] - 176s 7s/step - loss: 2.7105 - val_loss: 2.7127\n",
            "Epoch 16/100\n",
            "26/26 [==============================] - 174s 7s/step - loss: 2.7592 - val_loss: 2.6973\n",
            "Epoch 17/100\n",
            "26/26 [==============================] - 171s 7s/step - loss: 2.7348 - val_loss: 2.6904\n",
            "Epoch 18/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.6927 - val_loss: 2.6979\n",
            "Epoch 19/100\n",
            "26/26 [==============================] - 169s 7s/step - loss: 2.5418 - val_loss: 2.6756\n",
            "Epoch 20/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.6167 - val_loss: 2.6443\n",
            "Epoch 21/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.6123 - val_loss: 2.6277\n",
            "Epoch 22/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.5533 - val_loss: 2.6306\n",
            "Epoch 23/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.6277 - val_loss: 2.6042\n",
            "Epoch 24/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.5537 - val_loss: 2.6018\n",
            "Epoch 25/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.5152 - val_loss: 2.5777\n",
            "Epoch 26/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 2.4695 - val_loss: 2.5861\n",
            "Epoch 27/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.3537 - val_loss: 2.5477\n",
            "Epoch 28/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.4305 - val_loss: 2.5386\n",
            "Epoch 29/100\n",
            "26/26 [==============================] - 166s 6s/step - loss: 2.3652 - val_loss: 2.5530\n",
            "Epoch 30/100\n",
            "26/26 [==============================] - 169s 6s/step - loss: 2.3694 - val_loss: 2.5395\n",
            "Epoch 31/100\n",
            "26/26 [==============================] - 168s 6s/step - loss: 2.2659 - val_loss: 2.5143\n",
            "Epoch 32/100\n",
            "26/26 [==============================] - 175s 7s/step - loss: 2.2304 - val_loss: 2.5296\n",
            "Epoch 33/100\n",
            "26/26 [==============================] - 173s 7s/step - loss: 2.2907 - val_loss: 2.4972\n",
            "Epoch 34/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.2512 - val_loss: 2.4924\n",
            "Epoch 35/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 2.2507 - val_loss: 2.4979\n",
            "Epoch 36/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 2.2251 - val_loss: 2.4702\n",
            "Epoch 37/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 2.1851 - val_loss: 2.4751\n",
            "Epoch 38/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 2.1817 - val_loss: 2.4787\n",
            "Epoch 39/100\n",
            "26/26 [==============================] - 170s 7s/step - loss: 2.1547 - val_loss: 2.4713\n",
            "Epoch 40/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.1368 - val_loss: 2.4721\n",
            "Epoch 41/100\n",
            "26/26 [==============================] - 174s 7s/step - loss: 2.1332 - val_loss: 2.4764\n",
            "Epoch 42/100\n",
            "26/26 [==============================] - 171s 7s/step - loss: 2.0803 - val_loss: 2.4751\n",
            "Epoch 43/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.0684 - val_loss: 2.4556\n",
            "Epoch 44/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 1.9910 - val_loss: 2.4805\n",
            "Epoch 45/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.0577 - val_loss: 2.4431\n",
            "Epoch 46/100\n",
            "26/26 [==============================] - 172s 7s/step - loss: 2.0508 - val_loss: 2.4439\n",
            "Epoch 47/100\n",
            "26/26 [==============================] - 170s 7s/step - loss: 1.9437 - val_loss: 2.4426\n",
            "Epoch 48/100\n",
            "26/26 [==============================] - 166s 6s/step - loss: 1.9525 - val_loss: 2.4510\n",
            "Epoch 49/100\n",
            "26/26 [==============================] - 167s 6s/step - loss: 1.9608 - val_loss: 2.4374\n",
            "Epoch 50/100\n",
            "26/26 [==============================] - 170s 7s/step - loss: 1.9239 - val_loss: 2.4425\n",
            "Epoch 51/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYZz-fKks9YX"
      },
      "source": [
        "#plot training\r\n",
        "from matplotlib import pyplot\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPLN02pnShhK"
      },
      "source": [
        "It is interesting to note, that continuting training does increase document summary \"legibility\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q508-rzs1Hz"
      },
      "source": [
        "## Encoder and Decoder\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3-CBcmTs8SQ"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\r\n",
        "reverse_source_word_index=x_tokenizer.index_word\r\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr-cO9j5tB5t"
      },
      "source": [
        "# Encode the input sequence to get the feature vector\r\n",
        "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\r\n",
        "\r\n",
        "# Decoder setup\r\n",
        "# Below tensors will hold the states of the previous time step\r\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\r\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\r\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\r\n",
        "\r\n",
        "# Get the embeddings of the decoder sequence\r\n",
        "dec_emb2= dec_emb_layer(decoder_inputs) \r\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\r\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\r\n",
        "\r\n",
        "#attention inference\r\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\r\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\r\n",
        "\r\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\r\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat) \r\n",
        "\r\n",
        "# Final decoder model\r\n",
        "decoder_model = Model(\r\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\r\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svXC7jhPtEuv"
      },
      "source": [
        "def decode_sequence(input_seq):\r\n",
        "    # Encode the input as state vectors.\r\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\r\n",
        "    \r\n",
        "    # Generate empty target sequence of length 1.\r\n",
        "    target_seq = np.zeros((1,1))\r\n",
        "    \r\n",
        "    # Populate the first word of target sequence with the start word.\r\n",
        "    target_seq[0, 0] = target_word_index['sostok']\r\n",
        "\r\n",
        "    stop_condition = False\r\n",
        "    decoded_sentence = ''\r\n",
        "    while not stop_condition:\r\n",
        "        #print('here')\r\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\r\n",
        "\r\n",
        "        # Sample a token\r\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\r\n",
        "        #print(sampled_token_index)\r\n",
        "\r\n",
        "    #***for some reason the decoder breaks when sampled_token_index=0 t\r\n",
        "        if(sampled_token_index == 0):sampled_token_index = sampled_token_index+1\r\n",
        "    #****\r\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\r\n",
        "        #sampled_token = reverse_source_word_index[sampled_token_index]\r\n",
        "        \r\n",
        "        if(sampled_token!='eostok'):\r\n",
        "            decoded_sentence += ' '+sampled_token\r\n",
        "\r\n",
        "        # Exit condition: either hit max length or find stop word.\r\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\r\n",
        "            stop_condition = True\r\n",
        "\r\n",
        "        # Update the target sequence (of length 1).\r\n",
        "        target_seq = np.zeros((1,1))\r\n",
        "        target_seq[0, 0] = sampled_token_index\r\n",
        "\r\n",
        "        # Update internal states\r\n",
        "        e_h, e_c = h, c\r\n",
        "\r\n",
        "    return decoded_sentence\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfzyTkwBtIxH"
      },
      "source": [
        "def seq2summary(input_seq):\r\n",
        "    newString=''\r\n",
        "    for i in input_seq:\r\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\r\n",
        "            newString=newString+reverse_target_word_index[i]+' '\r\n",
        "    return newString\r\n",
        "\r\n",
        "def seq2text(input_seq):\r\n",
        "    newString=''\r\n",
        "    for i in input_seq:\r\n",
        "        if(i!=0):\r\n",
        "            newString=newString+reverse_source_word_index[i]+' '\r\n",
        "    return newString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmgj17pMtMyi"
      },
      "source": [
        "## Generated Abstractive Summaries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmljJiLQtMF_"
      },
      "source": [
        "for i in range(0,10):\r\n",
        "    print(\"Review:\",seq2text(x_tr[i]))\r\n",
        "    print(\"Original summary:\",seq2summary(y_tr[i]))\r\n",
        "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\r\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMjXxl4-XUNF"
      },
      "source": [
        "# Extractive Summarizer\r\n",
        "This is an entirely different approach to text processing for creating summaries. Instead of using an abstractive layer to create original text to then turn into a summary, the extractive approach instead takes all of the sentences in the original article and places them into a probability matrix to calculate how often a sentence that is simailar to itself appears in the document. After that process you simply print out the top n sentences and combine them to create a summary.\r\n",
        "\r\n",
        "This approach has the benefit of creating sentences that are grammatically correct, with the downfall being that the transitions between sentences might be completely missing or nonsensical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xi9wQ8SZSvq",
        "outputId": "2cb92433-9e55-41da-c292-d6afba690e1e"
      },
      "source": [
        "# download pretrained GloVe word embeddings\r\n",
        "! wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "nltk.download('stopwords')# one time execution\r\n",
        "from nltk.corpus import stopwords\r\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-18 01:16:36--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-18 01:16:36--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-18 01:16:37--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.86MB/s    in 7m 29s  \n",
            "\n",
            "2020-12-18 01:24:06 (1.83 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WloM3dqZsE-",
        "outputId": "dc0c1ea6-985f-4813-f44c-ab012f6d3164"
      },
      "source": [
        "df = data\r\n",
        "! unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR3tkoAdZvyx"
      },
      "source": [
        "## Extractive Summarizer Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N7RDWBAaAyu"
      },
      "source": [
        "  # Extract word vectors\r\n",
        "  word_embeddings = {}\r\n",
        "  f = open('glove.6B.100d.txt', encoding='utf-8')\r\n",
        "  for line in f:\r\n",
        "      values = line.split()\r\n",
        "      word = values[0]\r\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "      word_embeddings[word] = coefs\r\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F240fTAAaBlI"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import networkx as nx\r\n",
        "\r\n",
        "# function to remove stopwords\r\n",
        "def remove_stopwords(sen):\r\n",
        "  sen_new = \" \".join([i for i in sen if i not in stop_words])\r\n",
        "  return sen_new\r\n",
        "\r\n",
        "def summarizer(text, original_length=10000, summary_length=2000, max_sentences=10,article_num=-1):\r\n",
        "\r\n",
        "  # split the the text in the articles into sentences\r\n",
        "  sentences = []\r\n",
        "  for s in text[:1]:\r\n",
        "    sentences.append(sent_tokenize(s)) \r\n",
        "\r\n",
        "  # flatten the list\r\n",
        "  sentences = [y for x in sentences for y in x]\r\n",
        "\r\n",
        "\r\n",
        "  # remove punctuations, numbers and special characters\r\n",
        "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\r\n",
        "\r\n",
        "  # make alphabets lowercase\r\n",
        "  clean_sentences = [s.lower() for s in clean_sentences]\r\n",
        "\r\n",
        "  # remove stopwords from the sentences\r\n",
        "  clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]\r\n",
        "\r\n",
        "  sentence_vectors = []\r\n",
        "  for i in clean_sentences:\r\n",
        "    if len(i) != 0:\r\n",
        "      v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\r\n",
        "    else:\r\n",
        "      v = np.zeros((100,))\r\n",
        "    sentence_vectors.append(v)\r\n",
        "\r\n",
        "  # The next step is to find similarities among the sentences. We will use cosine similarity to find similarity between a pair of sentences. Let's create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.\r\n",
        "\r\n",
        "  # similarity matrix\r\n",
        "  sim_mat = np.zeros([len(sentences), len(sentences)])\r\n",
        "\r\n",
        "  for i in range(len(sentences)):\r\n",
        "    for j in range(len(sentences)):\r\n",
        "      if i != j:\r\n",
        "        sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\r\n",
        "\r\n",
        "\r\n",
        "  nx_graph = nx.from_numpy_array(sim_mat)\r\n",
        "  scores = nx.pagerank(nx_graph)\r\n",
        "  \r\n",
        "  # order the sentences in the order of the ranks of the rank matrix\r\n",
        "  ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\r\n",
        "\r\n",
        "  # Generate output\r\n",
        "  original = \"\"\r\n",
        "  for i in sentences:\r\n",
        "    original = original + \" \" + i\r\n",
        "\r\n",
        "  if(len(original) > original_length):\r\n",
        "    original = original[:original_length] + \"...\"\r\n",
        "\r\n",
        "  display(Markdown('## Article #'+ str(article_num) + ':'))\r\n",
        "  display(Markdown(original))\r\n",
        "\r\n",
        "  # Generate summary\r\n",
        "  display(Markdown('## Summary:'))\r\n",
        "  summary = \"\"\r\n",
        "  for i in range(max_sentences):\r\n",
        "    summary = summary + \" \" + ranked_sentences[i][1]\r\n",
        "\r\n",
        "  if(len(summary) > summary_length):\r\n",
        "    summary = summary[:summary_length] + \"...\"\r\n",
        "\r\n",
        "    \r\n",
        "  display(Markdown(summary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3NXUtjl2aIw3",
        "outputId": "c2da5e6a-864f-4e6e-9bc7-3388aa192331"
      },
      "source": [
        "for i in range(20):\r\n",
        "  if(i !=2):\r\n",
        "    summarizer(df[\"Text\"][i:i+1], original_length=2000, summary_length=400, max_sentences=5,article_num=i+1)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #1:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Use of Personal Protective Equipment (PPE) has been a central behavioural and policy response to control spread of the SARS-CoV2 virus during the global COVID-19 pandemic. In particular masks, and sometimes gloves, aprons, gowns, and face/eye protection have been recommended or used in high-risk situations such as healthcare settings or enclosed public spaces. The resultant surge in demand for PPE has required an increase in PPE production, including an estimated 11% increase in global production of gloves this year. (1)Whereas there is evidence that PPE is effective in limiting transmission of the SARS-CoV2 virus, the necessity and extent of PPE for use in different circumstances is still subject to debate. (2) Excessive use of PPE risks generating unnecessary financial cost: for example, by early July 2020 the UK government had allocated GBP £15bn of funds for purchasing PPE for public sector workers. (3) In addition, use of PPE generates a cost to the environment (which in turn impacts on human health), but to date that risk has not been quantified.Here we use the approach of life cycle assessment (LCA) to estimate emissions and resulting environmental impact from the most common PPE items prescribed and used in the National Health Service (NHS) and public social care sector in England: masks, gloves, aprons, gowns, and face/eye protection. (4) We equate this with data on the volumes of these products supplied to health and social care services in England in the first six months of the COVID-19 pandemic, to estimate the overall environmental impact of PPE over this time period. We evaluate the associated damage to human health (measured in disability adjusted life years), ecosystems (loss of local species), and resource scarcity (financial cost involved in future mineral and fossil resource extraction). (5) We model a number of approaches which could reduce such impact, and which could inform future policy on use and supply of PPE.We based our analysis on produc...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " CO 2 e= carbon dioxide equivalents, FFP= filtering facepieceEnvironmental impacts (endpoint categories) of alternative scenarios, modelled on total volumes of core PPE supplied to health and social care services in England between 25 th February and 23 rd August 2020, normalised to highest scenario for each impact factor, modelling base scenario (air freight, single-use PPE, clinical waste), use ...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #2:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " We developed a deterministic compartmental transmission model of SARS-CoV-2 in a population stratified by disease status (susceptible, exposed, infectious with mild or severe disease, diagnosed, and recovered) and disease awareness status (aware and unaware) due to the spread of COVID-19. Self-imposed measures were assumed to be taken by diseaseaware individuals and included handwashing, mask-wearing, and social distancing. Government-imposed social distancing reduced the contact rate of individuals irrespective of their disease or awareness status. The model was parameterized using current best estimates of key epidemiological parameters from COVID-19 clinical studies. The model outcomes included the peak number of diagnoses, attack rate, and time until the peak number of diagnoses. For fast awareness spread in the population, self-imposed measures can significantly reduce the attack rate and diminish and postpone the peak number of diagnoses. We estimate that a large epidemic can be prevented if the efficacy of these measures exceeds 50%. For slow awareness spread, self-imposed measures reduce the peak number of diagnoses and attack rate but do not affect the timing of the peak. Early implementation of shortterm government-imposed social distancing alone is estimated to delay (by at most 7 months for a 3-month intervention) but not to reduce the peak. The delay can be even longer and the height of the peak can be additionally reduced if this intervention is combined with self-imposed measures that are continued after government-imposed social distancing has been lifted. Our analyses are limited in that they do not account for stochasticity, demographics, heterogeneities in contact patterns or mixing, spatial effects, imperfect isolation of individuals with severe disease, and reinfection with COVID-19.Our results suggest that information dissemination about COVID-19, which causes individual adoption of handwashing, mask-wearing, and social distancing, can be an e...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " This means that, for fast spread of awareness, a large outbreak can be prevented by, for example, a combination of handwashing and self-imposed social distancing, each with an efficacy of around 25% (or other efficacies adding up to 50%).For many countries around the world, the focus of public health officers in the context of COVID-19 epidemic has shifted from containment to mitigation and delay...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #4:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Estimating the field effectiveness of influenza vaccines (VE) poses specific challenges for the 2009 A(H1N1) pandemic. In particular, both pandemic and seasonal vaccination campaigns took place during the epidemic and, as a consequence, vaccine coverage changed through time, both in influenza cases and in the population as a whole.In France, pandemic vaccination conformed to a priority list established by public health authorities based on exposure and/or transmission probability, or on risk of complication subsequent to influenza [1] . The priority allocation of pandemic vaccines is shown in Figure 1 , along with the evolution of vaccine coverage over time, by broad age categories. Medical and paramedical staffs working in hospitals were first called, on October 20 th (week 43). Individuals working with ambulatory patients presenting with influenza or working with patients at high risk of complication for influenza were called on November 2 nd (week 45). Risk factors of complication, stated in a High Committee of Public Health advice, on September 7 th 2009, were: pregnancy (in particular from the second trimester), obesity, and chronic conditions such as broncopulmonary diseases, heart diseases, diabetes and immunosuppression [2] . On November 12 th (week 46) all other health care professional were called (880,000), as well as all persons in contact with infants younger than six month-old (1,200,000), childminders working with children under three year-old (500,000), and every person between six months and 64 years of age with a risk factor (2,815,000). Pregnant women from their second trimester and 6-to 23 month-old children without risk factor were called on November 20 th (week 47). High school pupils were called on November 25 th (week 48). People over 65 year-old with a risk factor (3, 200 ,000) and children older than 23 month-old (7,700,000) and were called in week 49. Finally, adults over 18 year-old without a risk factor were called in week 53 (39,000,00...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " A subsequent paper evidenced a moderate effectiveness of the 2008-2009 seasonal vaccines against pandemic A(H1N1) mild outcomes (VE = 42%, 29-53%) [31] , while another one put forward an increased risk (odds-ratio = 2.45, 1.34-4.48) [32] .Regarding the effectiveness of the 2009-2010 seasonal vaccines, most works found no protection against confirmed pandemic A(H1N1) [8, 23, [33] [34] [35] [36] , ...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #5:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " COVID-19 pandemic reached 3.78 million confirmed reported cases worldwide, and it is generally associated to the acronym that precedes its name: severe acute respiratory syndrome (SARS). However, the bottom of the iceberg is being progressively unveiled since it is far more than simply a severe interstitial pneumonia. There is a gap in knowledge of pathophysiological process that allows COVID-19 to be considered a multi-organ disease in all respects.We looked for main papers about SARS and COVID-19 as systemic diseases, focusing on cardiovascular, renal, liver, nervous, and reproductive systems.The SARS-COV-2 virus not only causes viral pneumonia; however, it also has major implications for the cardiovascular system, but the extent, severity, and duration are still to be defined. In a study of 75 patients, two patients had succumbed to acute myocardial infarction [1] .A study from Huang et al. demonstrated that myocardial injury, defined by an increase in hs-cTnI levels (> 28 pg/mL), occurred on 5 out of 41 COVID-19 patients. Noteworthy is the fact that 80% of these patients required ICU management demonstrating that myocardial damage in SARS-COV-2 infection is severe [2] .A prospective study investigating left ventricular performance in 46 patients with severe acute respiratory syndrome showed subclinical diastolic impairment without systolic involvement [3] . Another study in 121 patients with SARS identified cardiovascular complications including tachycardia (72%), hypotension (50%), bradycardia (15%), transient cardiomegaly (11%), and transient paroxysmal atrial fibrillation in only 1 patient, although usually self-limiting [4] . A study from Singapore reported postmortem examinations: the presence of pulmonary embolism (PE) and deep vein thrombosis and acute myocardial infarction is of great clinical interest, but the generalizability of this limited study is not established [5] . Anyway, it is well recognized that SARS-COV-2 infection alters coagulation pathw...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " ANE is a rare complication of influenza and other viruses resulting from intracranial cytokine storm and resulting in blood-brain barrier damage, plausible with COVID-19 pathogenetic signature.Although metabolic and electrolyte derangements, especially in thalamocortical pathways, secondary to COVID-19 are plausible causes of clinical or subclinical acute symptomatic seizures and status epileptic...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #6:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " On March 11 st , 2020, the World Health Organization (WHO) declared the ongoing outbreak of nCoV-2019 as a pandemic public health emergency. As of April 5 th , 2020, more than 1.200.000 confirmed SARS-CoV-2 cases have been reported globally. Of these, more than 124.000 were from Italy.The SARS-CoV-2 can manifest principally as severe pneumonia, Acute Respiratory Distress Syndrome (ARDS), and Multiple-Organ Failure (MOF), which can lead to death. The SARS-CoV-2 can be transmitted from human to human through respiratory droplets, contact, and even fecaloral transmission (1, 2) .Imaging plays an important role in the diagnosis and management of SARS-CoV-2, in particular in the case of pneumonia. Computed Tomography (CT) is considered the first-line imaging modality in highly suspected cases and helps to monitor pathological changes during treatment. Typical CT feature is the bilateral distribution of ground-glass opacities (GGOs) with or without consolidation in posterior and peripheral areas of lungs, as the cardinal hallmark of SARS-CoV-2 (3, 4, 5) .On the other hand, the routine chest X-ray is the most widely available radiological procedure during hospital admissions, in particular, to complete differential diagnosis of respiratory symptoms, such as cough and dyspnea. The chest radiograph can establish the presence of pneumonia, define its extension and location, and can also diagnose complications. However limited information exists regarding chest X-ray imaging findings of SARS-CoV-2 lung infection (6) .Overall, due to the increasing number of reported cases of SARS-CoV-2 infection, radiologists encounter more frequently patients in the emergency setting. The goal of the radiological emergency department is to quickly undertake the diagnosis and evaluation of patients with suspected SARS-CoV-2 and provide frontline diagnosis and confirmation of the disease. A chest Xray may help in early detection of lung abnormalities for screening outpatients with highly suspe...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Moreover, we found out that consolidation is statistically associated with age > 60 years old and not associated with symptoms onset (p<0.0001 and p=0.924 respectively), and, consistently with previous studies, SARS-CoV-2 was more often found in men than in women (9, 10, 11, 12, 13).A chest radiograph can establish the presence of pneumonia, define its extension and location, and can also diagnos...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #7:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Influenza viruses belong to the Orthomyxoviridae family, whose members are defined by a segmented, single-stranded, negative-sense RNA genome (which is replicated in the nucleus) and an envelope that is derived from the host cell [1] . Influenza A viruses have eight genome segments that encode for 10 or 11 viral proteins, depending on the strain. Nine of these proteins are found in the virion. These include: the HA, NA and M2 proteins, which are all inserted into the lipid envelope; the matrix (M1) protein which lies beneath the membrane; the NP which coats the viral genome; the polymerase complex (PB1, PB1, PA) which is associated with the encapsidated genome, and the nuclear export protein (NEP). The remaining viral proteins, NS1 and PB1-F2, are expressed in infected cells but are not packaged into the virus particle.Influenza virus initiates infection via attachment of HA to sialic acid-containing proteins on the host cell membrane (Figure 1 ). The virus particle then enters the cell by pH-dependent endocytosis, although there appears to be flexibility in the pathway that is used, with an estimated two thirds using a clathrin-dependent pathway and the remaining third entering via an undefined pathway that is independent of both clathrin and caveolin [2] . Once in the acid environment of the late endosome, the HA undergoes a conformational change and drives fusion of the viral envelope with that of the endosome [3] . In addition, the M2 protein, which has ion channel activity [4] , pumps H+ ions into the interior of the virion and this dissociates M1 from the viral ribonucleoprotein complexes (vRNPs). The released vRNPs enter the cytoplasm through the fusion pore and are transported into the nucleus via interaction of NP with karyopherin alpha proteins [5] , which are part of the nuclear import machinery. Once in the nucleus, the incoming viral polymerase complex initiates genome transcription. In a process known as \"cap-snatching\", the PB2 protein binds to the 5...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Some of these cellular factors may be specifically required by influenza virus, while others may also play a role in other virus infections, which presents the opportunity for development of a host-directed drug with broad-spectrum activity.RNA interference siRNA small inhibitory RNA vRNPs viral ribonucleoprotein complexes Representation of the number of common hits amongst five influenza virus R...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #8:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The COVID-19 pandemic is a rapidly evolving global emergency, which looks set to become a longterm challenge for society. Patients who are hospitalized with COVID-19 may be treated with multiple drugs, including experimental treatments and adjuvant therapies, in addition to any concomitant medications already prescribed to the patient. As such, it is vital for clinicians to be able to quickly and easily integrate as much information as possible, particularly about patient-specific factors, such as genetic variants, to optimize treatment.The Pharmacogenomics Knowledgebase (PharmGKB) has curated pharmacogenomic (PGx) knowledge since 2000 (1) . This wealth of information about the impact of genetic variation on drug response collected from research publications, regulator-approved drug labels, clinical guidelines and other sources is freely available to all through the PharmGKB website (https://www.pharmgkb.org). Although the majority of PharmGKB content is collected and curated manually by expert scientific curators, automated annotations of PubMed abstracts and full-text articles in PubMed Central can also be accessed by users (2) .In response to this pandemic, PharmGKB has produced a COVID-19 portal to bring together relevant pharmacogenomic resources from across the site as well as some links to external materials. The use of pharmacogenomic information should only form part of the prescribing decision-making process and must be considered alongside other factors, including concomitant medications and drug-drug interactions. This tutorial paper will guide users through the information presented in the COVID-19 portal and provide an introduction to navigating the PharmGKB website.The PharmGKB COVID-19 portal can be accessed at https://www.pharmgkb.org/page/covid. It is currently also available via the red button displayed on the PharmGKB homepage or users can search for COVID-19 in the search box and follow the link given on the COVID-19 disease page to theThis art...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Clicking on each resource name takes the user to an overview of all of the annotations of that type.Some experimental COVID-19 treatments do not currently have any pharmacogenomic information available, either because they are awaiting approval from the U.S. Food and Drug Administration (FDA) or because pharmacogenomic studies of these drugs have not yet been carried out. The portal is updated on...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #9:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Rudolf Virchow , one of the foremost 19th century German leaders in medicine and pathology [1] , noted a relationship between human diseases and animals and then introduced the term \"zoonosis\" (plural: zoonoses) in 1880 [2] . Later, the World Health Organization (WHO) in 1959 specified that \"zoonoses are those diseases and infections which are naturally transmitted between vertebrate animals and man\" [3] . Venkatesan and co-authors reported that the term zoonosis is derived from the Greek word \"zoon\" = animal and \"noso\" = disease [4] . Zoonotic pathogens causing different kinds of diseases are of major public health issues worldwide [5] . These zoonotic diseases includeFrequent mixing of different animal species in the markets in densely populated areas, and the human intrusions into the natural habitats of animals, have facilitated the emergence of novel viruses. The most important zoonotic viral diseases of which eight were diagnosed (in dead or diseased animals or through antibody detection) on the Arabian Peninsula over the last years include rabies, Middle East Respiratory Syndrome (MERS-CoV), influenza virus (IFV), Alkhurma hemorrhagic fever, Crimean-Congo hemorrhagic fever (CCHF), Rift Valley fever (RVF), West Nile fever (WNV), and dengue fever virus. Among these eight zoonotic viral diseases, two (Alkhurma and MERS-CoV) were first reported in a patient in 1994 and 2012, respectively in Saudi Arabia [33, 34] . These two were transmitted later to several other countries, not only in the Middle East but also to Africa, Asia, and Europe.Rabies is an almost invariably fatal zoonotic disease, which belongs to the genus Lyssavirus of the RNA family Rhabdoviridae. Rabies virus is considered an endemic viral infectious disease in animals in Saudi Arabia. Recent scientific data on rabies cases reported in camels at Al-Qassim region (one of the thirteen administrative regions of Saudi Arabia) showed that there is an increasing number of this fatal virus disease [35] ....",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " It was further suggested that the virus may have been introduced via unlawful entrance of viremic domestic or wild animals through the borders or through vectors that carry the virus into Turkey [209] .Camels play an important role in public health issues regarding zoonosis and they have been involved in most of the zoonotic infections which occurred in Saudi Arabia in the last three decades. How...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #10:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " At the time of writing, the COVID-19 pandemic had already caused more than half a million deaths worldwide; at the time of re-writing, some weeks later, this number was approaching one million. The effects of the virus have been widespread and substantial, from the collapse of healthcare systems [1] [2] [3] to the enforcement of isolation and quarantine. In the case of Nepal, the national lockdown lasted for 120 days uninterrupted [4] .In these circumstances, identifying reliable and effective disease control policies is of utmost importance. Here by \"policy\" we mean a deliberate intervention intended to mitigate the effects of a disease as it runs its course. In much of the mathematical literature on epidemiology, the process of generating a policy is as follows [5] : (1) based on their intuition, expert epidemiologists propose a number of suitable policies to control the disease; (2) the impact on the population of each of the considered policies is assessed through dynamical models of disease spread; (3) the outcomes of all policies are compared and a decision is taken as to which one is deemed to be the best.This three-step process has two disadvantages. First of all, the class of policies devised by an expert could well be suboptimal, since the optimal policy (under some figure of merit) could be extremely complicated and counterintuitive. Second, the method requires one to numerically simulate each policy. Given the exponential growth of the number of policies in the number of control parameters, the number of policies considered may be on the order of billions; hence, this strategy is not guaranteed to identify the optimal disease control policy in time to enforce it.In this paper we formulate the problem of identifying an optimal policy for disease control as an optimization problem. Using tools from optimization theory and machine learning [6] , we propose efficient heuristics to find optimal disease policies for a given model of a disease.To illustrate th...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Although in the following all our proposed policies are non-adaptive, the formalism we introduce allows one to optimize over adaptive policies as well.In conclusion, a disease control policy can always be identified with a vector function α of the time t and perhaps some other observed variables o, where each vector entry represents a type of government intervention at time t. In turn, we can use...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #11:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Respiratory viral infections are a significant cause of morbidity and mortality globally (Esposito et al., 2013; Huijskens et al., 2013) . Early identification of respiratory pathogens permits rapid implementation of appropriate infection control precautions, decreased antibiotic use and where appropriate, initiation of antiviral therapies (Barenfanger et al., 2000; Heinonen et al., 2011) . Traditional laboratory methods such as viral culture and direct fluorescent-antibody testing are time consuming and lack sensitivity, and are no longer the method of choice (Gharabaghi et al., 2011; She et al., 2010) . Currently, molecular methods are now standard and are employed in most virology laboratories. Multiplexed assays have enabled the detection of several viral targets and permit the simultaneous identification of co-infections in patient specimens (Esper et al., 2011) . There are currently several multiplex assays available commercially. Three such Health Canada approved assays include the Seeplex RV15 ACE Detection Kit (RV15) (Seegene, South Korea), the Anyplex II RV16 (RV16) (Seegene, South Korea) and the xTAG Respiratory Viral Panel (xTAG) (Luminex, United States)The RV15 is a multiplex assay based on dual priming oligonucleotide (DPO) technology (Bruijnesteijn van Coppenraet et al., 2010) . The list of fifteen detectable viruses include: influenza A virus (INF A), influenza B virus (INF B), respiratory syncytial viruses A and B (RSVA and RSVB), adenovirus (ADV), human metapneumovirus (hMPV), coronavirus OC43 (CoV OC43), parainfluenza viruses (PIV) 1-4, rhinovirus (RhV) A to C, enterovirus (EV), and Bocaviruses (BoV).The RV16 is based on Tagged Oligo Cleavage Extension (TOCE™) technology, which makes it possible to detect multiple pathogens in a single fluorescence channel using real time PCR (Kim et al., 2013) . The RV16 can detect a total of 16 viruses including serotypes of each virus. The RV16 viral panel is identical to the RV15 viral panel with the addition...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The Luminex xTag system is a liquid-bead-suspension-array that is based on multiplex PCR (Jokela et al., 2012) T bocaviruses were previously not included on the viral panel for xTAG; however, version 2 being analyzed in this study has this viral target as part of its testing panel.Thus, this study aims to compare the diagnostic performance of the RV16 and xTAG assays to that of the RV15, as the t...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #12:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " IMPORTANCE YFV-17D is a live attenuated flavivirus vaccine strain recognized as one of the most effective vaccines ever developed. However, the host and viral determinants governing YFV-17D attenuation and its potent immunogenicity are still unknown. Here, we analyzed the role of type III interferon (IFN)-mediated signaling, a host immune defense mechanism, in controlling YFV-17D infection and attenuation in different mouse models. We uncovered a critical role of type III IFN-mediated signaling in preserving the integrity of the blood-brain barrier and preventing viral brain invasion. Type III IFN also played a major role in regulating the induction of a potent but balanced immune response that prevented viral evasion of the host immune system. An improved understanding of the complex mechanisms regulating YFV-17D attenuation will provide insights into the key virus-host interactions that regulate host immune responses and infection outcomes as well as open novel avenues for the development of innovative vaccine strategies. KEYWORDS flavivirus, innate immunity, interferons, live vector vaccines, yellow fever virus A rthropod-borne flaviviruses such as dengue virus (DENV), yellow fever virus (YFV), and Zika virus (ZIKV) are a cause of major health concerns worldwide (1) (2) (3) . With a mortality rate of up to 25% to 50%, YFV infection is one of the most severe flavivirus infections and is responsible for around 200,000 new infections and 30,000 deaths each year (2) . The YFV live attenuated strain YFV-17D is one of the safest and most potent vaccines ever developed (4) . The vaccination of more than 500 million individuals over the past 80 years has significantly contributed to the prevention of YFV outbreaks (4) . However, vaccination coverage has decreased over the last few decades, and recent outbreaks in Africa and South America have highlighted the urgent need for reassessing strategies to control YFV infection worldwide (5) .The intricate interplay between YF...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Numbers of B cells were significantly reduced in the blood and liver of ␣␤R Ϫ/Ϫ R Ϫ/Ϫ mice, suggesting a potential effect on B-cell-mediated control of YFV-17D infection. Taken together, results from our study support a model in which severe viral neuroinvasion in ␣␤R Ϫ/Ϫ R Ϫ/Ϫ mice could be caused by the convergence of three major elements: active viral replication, increased BBB permeability, a...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #13:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " (1) Introduction:For centuries humanity had speculated about the existence of other planets orbiting other stars to our own. 25 years ago the discovery of a planet orbiting the solar-like star 51 Peg (Mayor & Queloz, 1995) opened the floodgates, leading to a vigorous expansion in the number of known exoplanets.51 Peg was the first widely accepted exoplanet discovered orbiting a solar-like star, using radial velocity measurements just as Struve (1952) had suggested could result in such a discovery of a massive planet close to its host: `A planet ten times the mass of Jupiter would be easy to detect, since it would cause the observed radial velocity of the star to oscillate with ± 2 km per second'. It is worth noting that 51 Peg was not the first exoplanet to be found. Radio pulsar timings by Wolszczan & Frail (1992) had provided convincing evidence to planetary masses outside our solar system. The radial velocity work of Latham et al. (1989) and Hatzes & Cochran (1993) had provided evidence suggesting objects of planetary masses orbiting main sequence stars. Similarly, Campbell et al. (1988) proposed a planetary candidate in a 2.7 year orbit in the binary Gamma Cephei system, based on a radial velocity curve of 25 metre per second amplitude. However and later cast doubt on the interpretation, noting that their velocity standards showed roughly annual variations that correlated with their chromospheric activity. Walker et al. (1992) refined the period to 2.52 years noting that \"an explanation… in terms of … a Jupiter-mass planet in a high circular orbit …is still viable\" but expressed concern that such a massive planet could truly exist so close to its host star. Indeed, the strange nature of 51 Peg's planet (an orbital radius some 5% that of Earth's, a 4.23 day orbit, and a mass similar to Jupiter's) raised eyebrows at the time of its discovery announcement. The field of planet discoveries had had many false alarms in the past, such as Peter van de Kemp's claims tha...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " A similar analysis would be needed for the radial velocity data and is something planned for the next iteration of the course at Harper College, now that we know that the students are able to reach this kind of analysi Even a cursory examination of the summary pages of the NEA show clearly the wide range of statistical error estimates for a given system, which can be orders of ten in size.Two stu...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #14:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Bioaerosols are present virtually anywhere in the environment, and their exposure is shown to cause numerous adverse health effects [1] [2] . In addition, there is also a possible release of biowarfare agents in a man-made bio-terror event. A number of studies demonstrated that the respiratory tract can be colonized with disease organisms [3] [4] [5] . Through talking, coughing, sneezing or singing, the potential virulent organisms can be exhaled and spread into the ambient environment [6] , which accordingly causes air contamination. For example, SARS in 2003 and H1N1 in 2009 outbreaks were shown to be attributed to the airborne route of disease transmission [7] [8] [9] [10] .Among many other diseases, respiratory infection accounts for 23.3-42.1% of the total hospital infections [11] , and is listed as the third leading killer [12] . However, present diagnosis procedures using nasal swabs, bronchoalveolar lavages, nasopharyngeal aspirates or sputum samples, appear to cause unpleasant experiences in addition to long detection time. During flu outbreaks, body temperature or isolation procedures are often used to control and prevent further spread, however such methods are lacking scientific evidence and not always effective with those patients infected but in latent period. On another front, exhaled breath condensate (EBC) as a simple and noninvasive method is increasingly being utilized in early disease screening and infectious aerosols measurements, e.g., lung cancer [13, 14] , asthma [15, 16] , and other respiratory problems [17, 18] . In previous studies, human influenza A viruses were detected in exhaled breath using EBC [19, 20] as well as filter [21] , mask [22, 23] and a liquid sampler [24] . In another study, foot-and-mouth disease viruses were also found in the exhaled air from experimentally infected cattle [25] . In addition, high levels of bacterial concentrations in EBC were also observed in other studies [26] [27] [28] [29] . It was recently shown th...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Results shown in Figures 5, 6 , 7, and 8 indicate that high levels of bacterial aerosols were detected in the EBC samples collected, and the results on the other hand also implied that the developed device was efficient in collecting bacterial particles in the exhaled breath. Experimental data here also suggest that exhaled breath, which was shown to contain smaller bacterial particles, could pla...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #15:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " R&D investment. Scherer (2010, p. 562) observes: \"Beginning already in the late 1950s, the drug makers were accused in public fora of profiteering at the expense of consumers. They argued in return that high profits were a reward for superior innovation and a necessary spur to investment in risky R&D.\" Thoughtful proposals have formulated policies that aim to balance the need for lower prescription drug prices and yet preserve incentives for pharmaceutical innovation. Frank and Nichols (2019, p. 1405 ) offer a price-negotiation proposal in which \"the fallback for failed negotiation would not be an arbitrarily low price dictated by the government that would threaten innovation. \"However, even price controls would not necessarily reduce R&D investment below a desirable level, and in this paper, we propose a royalties policy that would be an alternative to price controls and would be far less likely to adversely affect R&D investment. Indeed, the royalties policy would incentivize innovation by providing incentives for the creation of commercially useful biomedical inventions from fundamental research in universities and federal laboratories and also for the successful technology transfer of those inventions. Moreover, the royalties policy would be likely to lower the effective prices paid by the taxpayers.In Section 2, we explain the positive incentives that royalties provide for biomedical R&D performance. We also explain the reason that royalties would not be likely to reduce R&D investment below desirable levels. In Section 3, we describe the two avenues through which the taxpayers provide funds for biomedical R&D and show the sizes of the two avenues. In Section 4, we explain the royalties policy that would return royalties (beyond those currently paid on licenses of federal laboratories' patented inventions) to the government from the sales of biomedical products supported with public funding through either of the two avenues. We then illustrate the royalties po...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " As a practical matter, it would be impossible to set the royalty rate at the ideal level shown, but the point is that the royalty is to cover an opportunity cost of the product that uses the federal agency's transferred technology.The situation could be different, for example if the federal agency or the university gave an exclusive license to its industry partner that then competed with other fi...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #16:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The Coronavirus disease 2019 (COVID-19) pandemic caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has spread worldwide since its first recorded case in the city of Wuhan, China in December 2019. According to the Open Access *Correspondence: navin.hor@mahidol.ac.th 1 Department of Microbiology, Faculty of Medicine Siriraj Hospital, Mahidol University, Bangkok, Thailand Full list of author information is available at the end of the article Dashboard on August 31st, 2020 by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University, over 25 million people in more than 200 countries have been infected and killed more than 840,000 [1] [2] [3] . It is expected that these numbers continue to rise, especially in populous countries such as the United States, Brazil, and India. In Thailand, the first documented cases of COVID-19 were two Chinese tourists arriving from the city of Wuhan on January 8th and 13th, 2020, respectively. As of August 31st, 2020, there have been 3,412 confirmed COVID-19 cases with 58 deaths; 2,444 cases were from local transmission [4, 5] . The Thai government mandated a 14-day State Quarantine for all travelers entering Thailand from abroad. Since May 26th, 2020, no new local transmission cases were documented; new confirmed COVID-19 cases were people who have tested positive while in State Quarantine after returning from abroad [5] . SARS-CoV-2 infection causes asymptomatic and mild diseases more than severe pneumonia. Severe cases may develop acute respiratory distress syndrome (ARDS) and death with an average mortality rate of 6% (range 1-14.4%) [1, 3, 6] .The real-time reverse transcription-polymerase chain reaction (RT-PCR) assay, which is the current standard test for laboratory diagnosis of SARS-CoV-2 infection, requires at least four hours of operation performed by skilled technicians. Therefore, rapid and accurate tests for SARS-CoV-2 screening are essential to expedite disease prevention and...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Our batch of clinical specimens might generally have higher viral loads (low Ct-value) than that of the manufacturer's trial site, which enhanced the chance of antigen detection in our study.Of 60 RT-PCR-positive samples in our study, the sole false negative result was from the NP and throat swab of a female patient with pneumonia tested for SARS-CoV-2 antigen seven days after disease onset (RT-P...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #17:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " D e levensbedreigende complicatie van een infectie met een groep A-streptokok is het ontstaan van het toxische-shocksyndroom, dat zich manifesteert als hypotensie met het optreden van multiorgaanfalen. In dit artikel presenteren wij een patiënte met aspecifieke bekkenklachten die bleken te berusten op een toxischeshocksyndroom in combinatie met een compartimentsyndroom van de onderbenen. Voor het compartimentsyndroom onderging deze patiënte fasciotomie en uiteindelijk was zelfs een hysterectomie noodzakelijk. De ziektegeschiedenis illustreert het fulminante beloop van een necrotiserende wekedeleninfectie en onderstreept het belang van vroege herkenning.Patiënte, een 37-jarige, gezonde gravida 2 para 1, met in de obstetrische voorgeschiedenis een spontane partus, werd bij een zwangerschapsduur van 40 4/7 weken aan ons overgedragen wegens niet-vorderende ontsluiting. Om de weeënactiviteit te stimuleren kreeg zij oxytocine toegediend. Na een manoeuvre volgens McRoberts wegens schouderdystocie werd een meisje geboren van 4460 g met een Apgar-score van 9 na 1 min en 10 na 5 min. Op dag 6 post partum kwam patiënte naar de SEH met bekkenpijn en uitstraling van de pijn in beide benen. Zij was helder en reageerde adequaat, de ademhalingsfre-Achtergrond Het optreden van een toxische-shocksyndroom na infectie met een groep A-streptokok post partum is een gevreesde complicatie. De mortaliteit van een toxische-shocksyndroom met necrotiserende wekedeleninfectie bedraagt 30-50%.cAsus Wij presenteren een kraamvrouw met aspecifieke bekkenpijn als eerste uiting van een toxische-shocksyndroom ten gevolge van een infectie met groep A-streptokokken. De klinische achteruitgang van patiënte maakte hysterectomie noodzakelijk. In verband met een compartimentsyndroom van beide benen werd beiderzijds een 4-loge-fasciotomie verricht, later gevolgd door débridement-operaties met split-skin-grafts.conclusie Deze casus illustreert de ernstige complicaties van een groep A-streptokokkeninfectie. N...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " De ziektegeschiedenis illustreert het fulminante beloop van een necrotiserende wekedeleninfectie en onderstreept het belang van vroege herkenning.Patiënte, een 37-jarige, gezonde gravida 2 para 1, met in de obstetrische voorgeschiedenis een spontane partus, werd bij een zwangerschapsduur van 40 4/7 weken aan ons overgedragen wegens niet-vorderende ontsluiting. In een latere fase werd zij overgepl...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #18:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The coronavirus disease (COVID-19) pandemic outbreak started in china with patients who had pneumonia of unknown etiology [1] . Other symptoms presented were dry cough, sore throat, and diarrhea. [1] The etiologic agent was identified as an RNA virus belonging to the Coronaviridae family, being then named as Sars-cov-2.As the epidemic progressed, other manifestations were observed. Recalcati et al. [3] studied 88 patients admitted in Lecco Hospital, Lombardy, Italy. Of these, 18 patients had skin reactions, and in eight the lesions occurred before hospitalization. These reactions were divided by the authors into three forms: erythematous rash, widespread urticaria, and chickenpox-like vesicles [3] . In addition to this study, a case was reported by Joob et al. [4] in Thailand. In this case, the patient presented a rash and a low platelet count and was initially misdiagnosed as dengue. He evolved with respiratory symptoms, being referred to a tertiary hospital and tested positive for sars-cov2 [4] .Despite the existence of skin lesions descriptions, there is little information about their evolutions and pictures about them. Therefore, we report one case that has been following since the prodromal period until the resolution of symptoms.Female patient, 55 years old, intensive care physician, resident of the city of Recife-PE. She has hypothyroidism and overweight comorbidities. The patient had contact with an intensive care unit (ICU) patient previously tested positive for Sars-cov-2 on 03/22/2020. Five days later, few painful erythematous-edematous plaques appeared on the flexor face of forearms and leg extensors (picture 1). Some lesions evolved into bruises. She was treated with betamethasone cream 0.1% once a day with lesion resolution in 3 days.The patient had a second exposure to another ICU patient with COVID-19 on 04/05/2020. On 06/04/20, she had a fever, epistaxis, headache, myalgia, vomiting, and diarrhea. In the skin, she presented pruritic urticarial lesi...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " She used only oral zinc and was completely recovered after 10 days of the onset of systemic symptoms.The pandemic situation caused by COVID-19 is still recent and at the moment there is little information about the cutaneous conditions associated with the virus. He evolved with respiratory symptoms, being referred to a tertiary hospital and tested positive for sars-cov2 [4] .Despite the existence...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #19:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " Though discovered in 1939 in bovine milk, lactoferrin (Lf) was reported to be an iron-containing protein [1] . Then after, its structure and chemical properties were detailed in a study published in 1960 [2] . Subsequently, Lf was found to be related to the superfamily of iron-binding glycoproteins, namely transferrins [3] . Beyond this point, Lf emerged as a focus in many research disciplines, aiming to discover its multitude functions. In this regard, we carried out a search on PubMed library to assess the progress in Lf research, in particular, Lf-based nanoparticles in cancer therapy. Our search results showed an increasing pattern of research on Lf role in cancer therapy, since emerged as early as 1992 (Fig. 1) . Furthermore, the rising progress to the field of nanomedicine by 2005 dragged the researchers' attention towards Lf and its potential application as a drug nanocarrier, and more specifically, the implications of Lf nanoparticles in cancer therapy [4] . Notably, Lf research patterns reached its maximum interest in 2018, with about 21 published articles discussing various roles of Lf in cancer therapy, and 32 research papers about Lf nanoparticles of which 11 articles focusing on cancer therapeutics.Since Lf is a natural protein present in the milk, the chances of eliciting any adverse immune reaction would be minimal [5] . Primarily, the particles fabricated based on Lf protein prepared by mild methods that do not involve any chemical reactions. Studies of our laboratory and other research groups on Lf NPs have shown excellent safety properties even after systemically administered at high doses concentrations with the liver and hematological biochemical parameters are retained [6] .Lf is a red to salmon-pink whey protein with a large molecular size of ~80 KDa. It is found in milk and in a smaller percentage in bile and tears [7] . It has an isoelectric point (pI) of 8.0-8.5, hence it is positively charged at physiological pH 7.4 [8] .In 1984, the molec...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The Lf coating layer not only enhanced the drug targeting to tumor site but also prolonged its circulation by forming protein corona that can evade clearance by RES and showed acid responsive drug release at tumor microenvironment.In addition to its intrinsic anticancer effect, Lf was exploited as a hydrophilic shell of amphiphilic micelles co-encapsulating two water insoluble drugs rapamycin and...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Article #20:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " The novel 2019 coronavirus disease has led to a global pandemic with a spectrum of clinical manifestations among infected patients 1 . Cardiopulmonary injury related to this viral infection is now recognized as a common feature resulting in high mortality 2 . Prior cardiovascular comorbidities, such as obesity, diabetes, hypertension, and coronary artery disease (CAD), portend a poor prognosis amongst these patients 3 . An association of COVID-19 infection with venous thromboembolic events (VTEs) has increasingly been recognized by observational studies outside the United States (US) 4, 5 . This was further corroborated by high incidences of VTE findings, such as clinically overlooked pulmonary embolism, on autopsy 6 . COVID-19 creates a hypercoagulable milieu due to exaggerated inflammatory response and, thus, leads to a coagulation disarray, as evidenced by elevated inflammatory markers including D-dimer [7] [8] [9] . However, such a correlation has not been systemically studied or published to date.We present our experience, including demographics, clinical characteristics, and outcomes, in COVID-19 patients who also developed confirmed pulmonary embolisms during the course of their initial COVID-19 illness.Hospital, two tertiary medical centers within the Northwell Health system. Both A total of 101 patients were included in this study ( Figure 1 ). Basic demographics including mean age, gender and ethnicity distributions are presented in Table 1 . The most common comorbidities were hypertension, obesity, hyperlipidemia, diabetes mellitus, smoking history and previous VTE, which was similar to the larger Northwell experience 10 Average time from hospital admission to PE diagnosis was 5 days (± 6.1). At the time of PE diagnosis, tachycardia, hypotension, hypoxia and fever were most frequently reported physical exam findings (Table 2) . EKG rhythm at time of PE diagnosis was most commonly sinus tachycardia followed by atrial fibrillation or atrial flutter. The Mc...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "## Summary:",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": " To our knowledge, our study is the first to suggest that high PESI scores represent a poor prognostic indicator in COVID-19 patients who develop acute PE.We report an overall low incidence of DVT and ischemic stroke in our studied population, although diagnostic testing was limited in an effort to minimize personnel exposure and viral transmission. RV strain on CT was noted in approximately half ...",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa5RXwJRZ4Wj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqNBjbNq6U90"
      },
      "source": [
        "# Technical Challenges and Limitations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2n58_pe5A6h"
      },
      "source": [
        "## Compute and Data\n",
        "\n",
        "One of the main problems we had during this project was getting adequate compute resources to train our model. As mentioned earlier, preprocessing alone required large amounts of memory, and could crash the kernel if not done in batches. A second challenge presented itself during the actual training of the model. Depending on the hyperparameters used for max_text_len, min_text_len, and max_summary_len, and min_summary_len, and batch_size, the kernel would crash when trying to instantiate an epoch. Using Google's Colab Pro, (P100-16 GB and 26 GB of RAM) we were unable to train our model with hyperparameters of 6000, 800, 500, 100, and 32 respectively. We decided to scale up using AWS EC2 instances, and even with a c5.24xlarge (96 virtual CPUs AND 192 GB of RAM), it still took 6 hours per epoch for the previously mentioned hyperparameters. We ended settling for 3000, 400, 300, 100, and 16 on a c4.8xlarge. Using these hyperparameters it took 2 hours per epoch.\n",
        "\n",
        "One of the reasons why this model is so compute intensive is that we are using sparse categoricaly crossentropy as our loss function. As we will see in the demonstration below, SCC does not care about locality of the one-hot encoded value, only that the correct one-hot encoded value was predicted. So even if our tokenizer was sophisticated enough to place semantically similar words close together, it would not have helped our loss. This is also the reason why our epochs required so much memory to run. For an article of word-length 3000, we are tokenizing every word, into a one-hot encoded vector of vocabulary-sized length. The dimensions of our tensors get very large, very fast as we attempt to increase the amount of training data we feed our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZW3UQ6e5IFS",
        "outputId": "f91363bb-cdd3-4e8a-9beb-c0b7644558d3"
      },
      "source": [
        "# credit: code adapted from Keras Documentation\n",
        "# source: https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-function\n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "print('Keras Documentation Example')\n",
        "y_true = [1, 2]\n",
        "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "assert loss.shape == (2,)\n",
        "print(f\"loss: {loss.numpy()}\\n\")\n",
        "# array([0.0513, 2.303], dtype=float32)\n",
        "\n",
        "print('Manufactured Example')\n",
        "y_true = [0, 0, 0]\n",
        "y_pred = [[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1,0.1,0.8]]\n",
        "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
        "assert loss.shape == (3,)\n",
        "print(f\"loss: {loss.numpy()}\")\n",
        "# array([0.0513, 2.303], dtype=float32)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keras Documentation Example\n",
            "loss: [0.0513 2.3026]\n",
            "\n",
            "Manufactured Example\n",
            "loss: [0.2231 2.3026 2.3026]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNCw6axzAx4r"
      },
      "source": [
        "## Framework and Skill\n",
        "\n",
        "We discovered many features of Keras/TensorFlow towards the end of our project that we would liked to have implemented at the beginning. One of the most useful features would have been using tf.keras.callbacks.ModelCheckpoint during training. This feature would have allowed to use to periodically write our model to file and continue training it at a later time. One of the questions we had during the project was if our predictions were bad because of our model or if they were bad because of a lack of training. By using the ModelCheckpoint we could have broken up training into smaller batches and trained overnight for multiple sessions, eventually consuming all of our training data.\n",
        "\n",
        "Another feature that we wanted to implement was the Bidirectional layer. We actually got our model to train using a Bidrectional layer, but then had trouble creating predictions and getting our data into the correct dimensions. If we had more time, I think we would have overcome this obstacle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgvf3uUH6Gvi"
      },
      "source": [
        "## Linguistics and Loss\n",
        "\n",
        "Another limitation of our model is that our predictions are only trained to match the summary word for word as best they can. Because we are using sparse categorical crossentropy, no points are awared for semantically similar phrases. One approach we experimented with, was Bilingual Evaluation Understudy (BLEU) scores. A BLEU score is used for comparing a candidate translation of text to one or more reference translations, and can be applied to any amount of cumulative n-grams. As we see in the example below, a semantically similar phrase will be given a better score than a non-sensical phrase. Although we can't directly inject a BLEU score calculation into our loss function, we might have been able to add another layer that performs a BLEU score calculation on the predicted summary and the target summary, and feed that result back into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k1GbJXq-Dbp",
        "outputId": "b8ac3389-4406-4736-d342-9f34d374a7df"
      },
      "source": [
        "# credit: code adapted from Jason Brownlee PhD\n",
        "# source: https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "\n",
        "def print_bleu_report(candidate, reference):\n",
        "    print(f\"candidate: {' '.join(candidate)}\")\n",
        "    print(f\"reference: {' '.join(reference[0])}\")\n",
        "    print(f\"cumulative 1-gram score: {sentence_bleu(reference, candidate, weights=[1]):.2f}\")\n",
        "    print(f\"cumulative 2-gram score: {sentence_bleu(reference, candidate, weights=[0.5, 0.5]):.2f}\")\n",
        "    print(f\"cumulative 3-gram score: {sentence_bleu(reference, candidate, weights=[0.33, 0.33, 0.33]):.2f}\")\n",
        "    print(f\"cumulative 4-gram score: {sentence_bleu(reference, candidate, weights=[0.25, 0.25, 0.25, 0.25]):.2f}\\n\")\n",
        "\n",
        "\n",
        "perfect_candidate = ['this', 'is', 'a', 'test']\n",
        "similar_candidate = ['this', 'is', 'small', 'test']\n",
        "print_bleu_report(perfect_candidate, [perfect_candidate])\n",
        "print_bleu_report(similar_candidate, [perfect_candidate])    \n",
        "\n",
        "perfect_candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
        "similar_candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
        "print_bleu_report(perfect_candidate, [perfect_candidate])\n",
        "print_bleu_report(similar_candidate, [perfect_candidate])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "candidate: this is a test\n",
            "reference: this is a test\n",
            "cumulative 1-gram score: 1.00\n",
            "cumulative 2-gram score: 1.00\n",
            "cumulative 3-gram score: 1.00\n",
            "cumulative 4-gram score: 1.00\n",
            "\n",
            "candidate: this is small test\n",
            "reference: this is a test\n",
            "cumulative 1-gram score: 0.75\n",
            "cumulative 2-gram score: 0.50\n",
            "cumulative 3-gram score: 0.63\n",
            "cumulative 4-gram score: 0.71\n",
            "\n",
            "candidate: the quick brown fox jumped over the lazy dog\n",
            "reference: the quick brown fox jumped over the lazy dog\n",
            "cumulative 1-gram score: 1.00\n",
            "cumulative 2-gram score: 1.00\n",
            "cumulative 3-gram score: 1.00\n",
            "cumulative 4-gram score: 1.00\n",
            "\n",
            "candidate: the fast brown fox jumped over the lazy dog\n",
            "reference: the quick brown fox jumped over the lazy dog\n",
            "cumulative 1-gram score: 0.89\n",
            "cumulative 2-gram score: 0.82\n",
            "cumulative 3-gram score: 0.78\n",
            "cumulative 4-gram score: 0.75\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPutQj0b_gS7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}